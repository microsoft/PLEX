## Overview


## Installation

*NOTE:* this setup has been tested on Windows 11's WSL2 running Ubuntu 18.04 LTS and 20.04 LTS, as well as on native Ubuntu 18.04 LTS and 20.04 LTS.

Install [MuJoCo 2.1 binaries](https://github.com/openai/mujoco-py#install-mujoco). TLDR:

```
# Download and extract the binaries:
wget https://mujoco.org/download/mujoco210-linux-x86_64.tar.gz -O mujoco.gz
mkdir ~/.mujoco
tar -xf mujoco.gz --directory ~/.mujoco
rm mujoco.gz

# Add these lines to your ~/.bashrc:
EXPORT LD_LIBRARY_PATH=$HOME/.mujoco/mujoco-210/bin:$LD_LIBRARY_PATH
EXPORT MJLIB_PATH=$HOME/.mujoco/mujoco-210/bin/libmujoco210.so

# Reload bashrc:
source ~/.bashrc
```

Create a virtual environment and install the PLEX repo there:

```
conda create -n plex pip=21.2.4 python=3.8
conda activate plex
conda install cudatoolkit=11.7 -c nvidia
git clone https://github.com/microsoft/PLEX.git
cd PLEX
pip install -e .
```

Create a directory for all the data you will use for training and models you will produce:
```
mkdir store
```

## Meta-World experiments

### Data setup

[The PLEX paper](https://arxiv.org/abs/2303.08789)'s Meta-World experiments use data that can be generated using scripted Meta-World policies with different levels of noise. These policies use a modified Meta-World flavor from the [PLEX-Metaworld](https://github.com/microsoft/PLEX-Metaworld) repo, which is installed automatically as one of PLEX's dependencies above.

To generate this training data, run the command below from the PLEX repo root. It will generate 100 trajectories for each of the 50 Meta-World tasks and extra 75 trajectories for 5 of these tasks, so it will take some time:

```
python scripts/gen_MW_data_for_PLEX.py --out=store/data/metaworld
```

The data will be placed in the `store` directory you created during PLEX setup.


### Running the PLEX paper's Meta-World experiments

#### First, pretrain EX.

To do so, run the following from the PLEX repo root:

```
python scripts/exps_on_MW.py --training_stage=ex --data_dir=store/data/ --log_dir=store/logs
```

#### Next, complete PLEX pretraining by choosing a checkpoint with a pretrained EX and pretraining PL on top of it.

The checkpoints generated by the previous step's command contain pretrained EX versions, have file names starting with `pretr_EX__`, and can be found in a subdirectory of `<PLEX repo root>/store/logs` whose name is the timestamp when the command was run. Generally, choose the checkpoint from the latest iteration -- its name ends in `__latest.pt` As an example, the command below uses such a checkpoint from the log directory `store/logs/11-26-23_06.47.24_None`:
```
python scripts/exps_on_MW.py --training_stage=pl --data_dir=store/data/ --log_dir=store/logs --model_file=store/logs/11-26-23_06.47.24_None/pretr_EX__plK30_plL3_plH4_exK30_exL3_exH4_res84_bcTrue_la1_relposTrue__latest.pt
```

#### Finally, finetune a pretrained PLEX for a specific target task.

Pretrained PLEX checkpoints are contained in the `PLEX/store/logs` subdirectory generated by the previous step. Their file names start with `pretr_PLEX__`. As for the preceding step, usually the `...__latest.pt` is a good choice. To get the results in the PLEX paper, finetuning the chosen checkpoint needs to be repeated 10 times (i.e., for 10 seeds) for each of target tasks of Meta-World's ML50: `metaworld/hand-insert-v2/--TARGET_ROBOT--/noise0/`, `metaworld/door-lock-v2/--TARGET_ROBOT--/noise0/`, `metaworld/door-unlock-v2/--TARGET_ROBOT--/noise0/`, `metaworld/box-close-v2/--TARGET_ROBOT--/noise0/`, and `metaworld/bin-picking-v2/--TARGET_ROBOT--/noise0/`. To parallelize the evaluation rollouts, you can set the number of evaluation workers greater than 0. PLEX's results in the paper for each of the target tasks `T` were obtained by taking, for each seed, the maximum success rate `R` across all finetuning iterations and averaging `R` across 10 seeds.

Below is an example finetuning command that finetunes a pretrained PLEX from the previous step on the `hand-insert-v2/` task and runs evaluation episodes using 5 workers:

```
python scripts/exps_on_MW.py --training_stage=ft --data_dir=store/data/ --num_workers=5 --target_task=metaworld/hand-insert-v2/--TARGET_ROBOT--/noise0/  --log_dir=store/logs --model_file=store/logs/11-27-23_00.20.38_None/pretr_PLEX__plK30_plL3_plH4_exK30_exL3_exH4_res84_bcTrue_la1_relposTrue__latest.pt
```

**NOTE**: If running PLEX on CPU, set `--num_workers=0`. Running PLEX on CPU with `--num_workers` > 0 will throw a  `"To use CUDA with multiprocessing, you must use the 'spawn' start method"` error, and using the `spawn` method will throw another error.


## Citing PLEX and the accompanying data

If you find PLEX, its implementation, or the accompanying Robosuite dataset useful in your work, please cite it as follows:

```
@inproceedings{thomas2023plex,
  title={PLEX: Making the Most of the Available Data for Robotic Manipulation Pretraining},
  author={Garrett Thomas and Ching-An Cheng and Ricky Loynd and Felipe Vieira Frujeri and Vibhav Vineet and Mihai Jalobeanu and Andrey Kolobov},
  booktitle={CoRL},
  year={2023}
  eprint={2303.08789},
  archivePrefix={arXiv},
  primaryClass={cs.RO}
  url={https://arxiv.org/abs/2303.08789}
}
```

## Contributing

This project welcomes contributions and suggestions.  Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.

When you submit a pull request, a CLA bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

## Trademarks

This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft
trademarks or logos is subject to and must follow
[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).
Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.
Any use of third-party trademarks or logos are subject to those third-party's policies.
